###
### Main parameter of this model:
###

model_name: LSTM                   # Name of the model
sampling_rate: 30                             # Sampling rate in Hz

# Dataset of ground truth data (in folder 'GT_datasets')   Example: GT_dataset_GC6s_Chen
training_datasets:
- DS03-Cal520-m-S1
- DS04-OGB1-zf-pDp
- DS05-Cal520-zf-pDp
- DS06-GCaMP6f-zf-aDp
- DS07-GCaMP6f-zf-dD
- DS08-GCaMP6f-zf-OB
- DS09-GCaMP6f-m-V1
- DS10-GCaMP6f-m-V1-neuropil-corrected
- DS11-GCaMP6f-m-V1-neuropil-corrected
- DS12-GCaMP6s-m-V1-neuropil-corrected
- DS13-GCaMP6s-m-V1-neuropil-corrected
- DS14-GCaMP6s-m-V1
- DS15-GCaMP6s-m-V1
- DS16-GCaMP6s-m-V1
- DS17-GCaMP5k-m-V1
- DS18-R-CaMP-m-CA3
- DS19-R-CaMP-m-S1
- DS20-jRCaMP1a-m-V1
placeholder_1: 0       # protect formatting


# Noise levels for training (integers, normally 1-9)
noise_levels:
- 2
- 3
- 6

placeholder_2: 0       # protect formatting


# Standard deviation of Gaussian smoothing in time (sec)
smoothing: 0.05

# Smoothing kernel is symmetric in time (0) or is causal (1)
causal_kernel: 0


###
### Additional parameters for model specification:
###


windowsize: 64                   # Windowsize in timepoints
before_frac: 0.5                 # Fraction of timepoints before prediction point (0-1)

# Filter sizes for each convolutional layer
filter_sizes:
- 31
- 19
- 5

# Filter numbers for each convolutional layer
filter_numbers:
- 30
- 40
- 50

dense_expansion: 30              # For dense layer


loss_function: mean_squared_error     # gradient-descent loss function
optimizer: Adagrad                    #                  optimizer

nr_of_epochs: 5                 # Number of training epochs per model
ensemble_size: 1                 # Number of models trained for one noise level
batch_size: 8192                 # Batch size

###
### Information about status of fitting
###

training_finished: Yes           # Yes / No / Running
verbose: 1                       # level of status messages (0: minimal, 1: standard, 2: most, 3: all)

###
### Additional parameter not specified in template
###
